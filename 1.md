基于您提供的图片文档，这是一套完整的**直播业务系统**架构设计，涵盖了从底层的**回调处理**、中间的**API服务**到上层的**业务时序逻辑**。

以下是该系统的详细技术拆解：

### 1. 核心架构与数据流转 (回调系统)

这部分是系统的“心脏”，负责处理与云厂商（如腾讯云）的交互及状态同步。

* **异步解耦设计**：
* 系统接收到回调数据后，首先进行**数据验证**（验签）。验证失败记录异常日志；验证通过后，**异步写入 Kafka** 并立即向腾讯云返回成功响应。这种设计防止了业务处理阻塞回调响应，提高了系统的吞吐量。
* 后端系统作为消费者（Consumer），从 Kafka 读取消息并根据 `event_type` 分发处理。


* **五大事件处理流程**：
1. **推流（直播开始）**：
* 校验直播合法性（如是否被禁播）。
* 更新数据库状态为 `live`，记录开始时间。
* **Redis 操作**：写入在线直播列表（Set结构），更新主播状态。
* 通知：发送全站或粉丝开播通知。


2. **断流（直播结束）**：
* 计算直播时长 (`end_time - start_time`)。
* 更新数据库状态为 `ended`。
* **清理缓存**：移除 Redis 在线列表。
* **结算**：触发直播数据统计和付费结算（如有）。


3. **录制**：
* 解析文件信息并入库，生成回放地址，触发后续的转码或压缩任务。


4. **截图**：
* 更新直播间封面图，保存截图路径。


5. **审核**：
* 根据云端审核结果（正常/违规）进行自动处理。
* **分级处罚**：轻度（警告）、中度（暂停直播）、重度（永久封禁）。





### 2. 数据存储方案

文档明确了关系型数据库与缓存的分工：

* **MySQL (持久化存储)**：
* `live_streams`: 直播流基础信息。
* `live_events`: 回调事件日志（用于排查问题）。
* `live_records`: 录制文件记录。
* `live_audit_logs`: 审核记录。
* `live_statistics`: 统计数据。


* **Redis (高频热数据)**：
* `set:live:online`: 在线直播房间列表（便于快速检索）。
* `hash:anchor:status:{anchor_id}`: 主播实时状态。
* `lock:callback:{sequence}`: 回调处理锁（防止重复处理）。
* `temp:live:{stream_id}`: 临时数据缓存。



### 3. 微服务API设计

系统被拆分为两个主要服务领域：**ser-blive (管理后台)** 和 **ser-live (直播业务)**。

#### A. 管理后台服务 (ser-blive)

面向运营人员，侧重于**监管与控制**：

* **房间管理**：查询列表、查看流水/观众详情、**强制关播**、置顶房间。
* **公约与设置**：管理直播间公约配置。
* **审核与监管**：查看审核列表、执行审核操作、实时监管画面。
* **权限控制**：封禁/解封用户开播权限。

#### B. 直播业务服务 (ser-live)

面向C端用户（主播/观众），侧重于**交互与体验**：

* **主播端接口**：
* 核心：`/start` (开播), `/close` (关播), `/room/settings` (更新标题/封面)。
* 管理：踢人 (`/kick`)、禁言 (`/mute`)、设管 (`/set-admin`)。
* 数据：直播结算摘要 (`/summary`)。


* **观众端接口**：
* 核心：`/enter` (进房), `/exit` (退房), `/recommend` (获取推荐列表)。
* 互动：点赞、分享、关注、举报。


* **WebSocket**：
* `/ws/live/{session_id}`: 推送实时弹幕、礼物消息。
* `/ws/notification/{uid}`: 推送系统通知。


* **个人中心**：
* 查看观看历史、收入明细、粉丝/关注列表。



### 4. 关键业务时序逻辑 (Sequence Diagrams)

这部分定义了最核心的业务流转细节。

* **开启直播流程**：
1. App发起请求 -> 后端检查权限（是否被禁播）。
2. 生成唯一 `session_id` (雪花算法)。
3. **调用云厂商接口** (`CreateLiveStream`) 创建流。
4. 数据库插入记录（状态：准备中）。
5. Redis 缓存房间信息。
6. 返回推流地址给主播 App。
7. App 收到地址后推流，并延时通知后端更新状态为“直播中”。


* **进入房间流程**：
1. App发起请求 -> 校验是否被踢出（Redis `live:kick`） -> 校验是否在黑名单。
2. 检查房间状态（必须是 `live_status=1` 直播中）。
3. 获取播放地址（HLS/FLV/RTMP）。
4. 记录观众进入日志 (`live_audience`)。
5. **并发处理**：Redis 在线人数+1，加入 IM 聊天室。
6. 广播“用户进入”通知。


* **关闭直播流程 (双端逻辑)**：
* **主播主动关播**：
1. 调用云厂商 `StopLiveStream` 断流。
2. DB更新：状态改 `ended`，计算时长。
3. **清理资源**：删除 Redis 房间缓存、移除在线列表。
4. **异步结算**：统计观看人数、流水，生成结算报告。
5. 广播通知：告知房间内观众直播结束。


* **后台强制关播**：
1. 管理员发起 -> 强制断流。
2. 更新数据库状态（标记为管理关闭 `close_method=2`）。
3. 清理缓存并广播结束通知。




* **观众退出流程**：
1. App发起退出 -> 验证用户在房状态。
2. 调用断流连接（NotifyDisconnect）。
3. Redis 移除在线记录，在线人数-1。
4. 退出 IM 聊天室。
5. **异步更新**：计算本次观看时长，更新观众历史记录。



### 5. 稳定性与异常机制

文档中还专门提到了异常处理策略：

* **Kafka 消费失败**：设置自动重试3次。
* **数据库操作失败**：记录到死信队列（Dead Letter Queue），防止数据丢失。
* **网络异常**：记录详细日志并设定定时重试机制。

**总结建议**：
文档最后建议将系统微服务化，拆分为：**回调接收服务、事件处理服务、录制管理服务、审核处理服务、统计计算服务**。这种拆分有利于应对高并发场景，例如“统计计算”和“录制转码”通常是资源消耗密集型任务，将其剥离可以避免影响核心的“事件处理”和“直播推拉流”的稳定性。

这是一个非常扎实的直播系统设计。为了帮你在面试中条理清晰地展示这个项目，我将所有文档内容重组为**五个核心模块**。

建议你按照**“架构总览 -> 核心业务 -> 异步处理 -> 稳定性保障 -> 性能优化”**的逻辑进行陈述。

---

### 模块一：架构设计与服务拆分 (System Architecture)

**面试话术：** “为了实现职责分离和应对不同维度的并发压力，我们将系统拆分为两个核心微服务。”

1. **C端业务服务 (`ser-live`)**
* **定位**：面向高并发的用户流量。
* **核心功能**：处理主播开播/关播、观众进出房、实时互动（点赞/关注）、WebSocket 消息推送。
* **关键接口**：
* 主播侧：`/room/start` (开播), `/room/close` (关播), `/room/kick` (踢人)。
* 观众侧：`/room/enter` (进房), `/room/exit` (退房), `/recommend` (推荐流)。




2. **管理后台服务 (`ser-blive`)**
* **定位**：面向运营人员的强监管需求。
* **核心功能**：房间治理（强制关播/置顶）、内容审核、黑名单管理、全平台监控。
* **关键接口**：`/admin/live/close` (强制下播), `/admin/live/ban` (封禁), `/admin/audit` (审核)。



---

### 模块二：核心业务流程 (Core Business Logic)

**面试话术：** “直播业务最关键的是流状态的生命周期管理，我们设计了严格的时序逻辑来保证状态一致性。”

1. **直播生命周期 (Stream Lifecycle)**
* **开播 (Start)**：
* **双重鉴权**：先查DB权限，再查封禁状态。
* **原子性ID**：使用雪花算法生成全局唯一 `session_id`。
* **状态预热**：客户端拿到推流地址后，服务端状态标记为“准备中”，通过回调或延时确认后再转为“直播中”。


* **关播 (Close)**：
* **逻辑统一**：无论是主播主动关播还是后台强制关播，最终都收敛到同一个处理逻辑。
* **资源清理**：立即断流 -> 移除Redis在线列表 -> 广播结束通知。




2. **用户进出房 (User Journey)**
* **漏斗式校验**：进房时依次校验：是否被踢出 (Redis) -> 是否在黑名单 (DB) -> 房间是否存活。
* **并发处理**：校验通过后，并行执行：
* Redis `INCR` 增加在线人数。
* 加入 IM 聊天组。
* 广播 `audience_join` 消息。


* **时长统计**：退房时异步计算观看时长 (`now - enter_time`) 并落库。



---

### 模块三：回调系统与异步架构 (Async Processing & Callback)

**面试话术：** “为了解决云厂商回调高并发带来的压力，我们采用 Kafka 进行异步削峰。”

1. **架构设计**
* **流程**：接收回调 -> 验签 -> **立即写入 Kafka** (并响应 200 OK) -> 后端 Consumer 异步消费。
* **优势**：防止因业务逻辑复杂导致回调超时，保证了接口的高吞吐量。


2. **事件分发策略**
* 根据 `event_type` 将流量分发到不同处理器：
* **推流事件**：修改直播状态 -> 发送开播通知。
* **断流事件**：计算时长 -> 触发结算 -> 清理缓存。
* **录制/截图事件**：生成回放链接，更新封面图。
* **审核事件**：根据违规级别（轻/中/重）自动执行警告、禁播或封禁。





---

### 模块四：数据存储与稳定性 (Data & Reliability)

**面试话术：** “针对直播场景读多写少的特点，我们制定了多级缓存策略和异常兜底机制。”

1. **存储选型**
* **MySQL**：存储核心元数据（`live_streams`, `live_records`, `live_audit_logs`）。
* **Redis**：
* `Set`: 在线直播列表（支持快速随机推荐）。
* `Hash`: 主播实时状态、用户观看进度。
* `Lock`: 分布式锁 `lock:callback:{seq}` 防止回调重复处理。




2. **稳定性保障**
* **重试机制**：Kafka 消费失败自动重试 3 次。
* **死信队列**：数据库操作失败后，数据写入死信队列 (DLQ) 防止丢失。
* **安全加固**：使用 MD5 算法生成推拉流防盗链签名。



---

### 模块五：性能优化 (Performance Tuning)

**面试话术：** “为了降低数据库压力，我们对不同实效性的数据采用了分级缓存策略。”

**缓存 TTL 策略**：

* **实时性要求高**：在线用户列表 -> **实时更新**。
* **读极其频繁**：热门房间排行榜 -> 缓存 **10分钟** (允许短时延迟)。
* **相对静态**：房间基本信息 -> 缓存 **30分钟**。

---

### 💡 面试加分项 (建议补充的亮点)

在讲完以上模块后，你可以补充以下几个技术难点是如何解决的：

1. **如何防止“幽灵房间”？**
* *答：* 我们利用腾讯云的回调机制（推流/断流）作为状态的“最终裁决者”，并配合 Redis 的 TTL 过期机制，确保如果服务端异常宕机，房间状态也能最终恢复一致。


2. **关播结算数据量大怎么办？**
* *答：* 所有的结算逻辑（计算总时长、总流水）都是**异步执行**的。关播接口只负责“切断状态”，具体的报表生成由后台 Worker 慢慢处理，不阻塞主播的操作体验。


3. **微服务演进方向？**
* *答：* 目前是按业务分，未来计划将**“统计服务”**和**“审核服务”**单独拆分，因为统计涉及大量计算，审核涉及第三方延迟，拆分后能避免拖累核心交易链路。
