# 功能模块划分（按目录/职责）
## 1) 接入层（WebSocket Server）
- **`internal/websocket_server.go`**
    - 监听端口、`Upgrade` 成 WebSocket
    - 创建 `Link` 并进入收包循环
    - 启动 MQ 消费者（后端推送、扩容事件）
    - etcd 注册、续租、节点 load 上报
    - **优雅下线**（停止接入、降权/注销、连接迁移/关闭）

## 2) 连接抽象与连接管理（Link / LinkManager）
- **`internal/link/*` + `internal/link/manager.go`**
    - `Link`：封装 net.Conn + WS session，提供 `Send/Receive/Close`、超时、缓冲、重试等
    - `LinkManager`：
        - 按 `bizID-userID` 生成 linkID，维护连接表
        - 支持按用户查找连接（后端 push 定位）
        - 维护连接数 `Len()` 供 load 上报/调度
          -（项目里还有 GracefulClose/重定向相关逻辑，属于面试高频点）

## 3) 消息处理链（LinkEventHandler）
- **`internal/linkevent/*`**
    - `Handler`：单条上行（前端→网关→业务后端 gRPC）+ 下行 push + ACK + 重传管理
    - `BatchHandler + Coordinator`：将上行请求按 biz 聚合 **批量调用后端**
    - `LimitHandler`：消息限流（`golang.org/x/time/rate`），心跳放行，超限回“被限流”消息并中断链路
- 典型能力：
    - 编解码（`codec`）、加解密（`encrypt`）
    - 幂等去重（Redis `SetNX`，按 `bizID-key`）
    - 上行 ACK、下行 ACK、下行重传（`pushretry`）

## 4) 后端交互层（gRPC + MQ）
- **gRPC**（同步链路）
    - `BackendService.OnReceive`：前端上行消息转业务后端，带超时+退避重试
- **MQ 事件系统**（异步链路）
    - `internal/event/*`
    - `Consumer`：消费后端推送（`pushMessage`）等事件
    - `SendToBackendEventProducer / BatchSendToBackendEventProducer`：把上行消息（或批量）投递到 MQ（削峰填谷/解耦）

## 5) 注册发现与集群治理
- **ServiceRegistry（etcd）**
    - lease 注册/续租
    - 节点状态更新：`load = linkManager.Len()`
    - 优雅注销：降权→等待→删除（配合连接迁移）

## 6) 限流/灰度/扩缩容（稳定性与容量）
- **连接接入侧限流（灰度放量）**
    - `TokenLimiter` + `StartRampUp`：逐步增加可接入令牌
    - `ExponentialBackOff`：容量不足时临时拒绝并退避
- **扩缩容**
    - `scaleUp` 事件消费 + `DockerScaler`（从 wire 注入看）实现自动扩容与再均衡
- **WebhookServer**
    - 控制面入口（扩容、再均衡触发等）

---

# 系统设计（端到端数据流）
## A. 前端→网关→后端（上行）
1. `Accept` 前先拿连接令牌（灰度/容量控制）
2. `Upgrade` 成 WS，创建 `Link`（带超时、缓冲、单用户限流等）
3. 收到 payload：
    - `codec.Unmarshal` → `decrypt`
    - Redis `SetNX` 做幂等（重复 key 丢弃）
    - 按 `Cmd` 分发：
        - 心跳：原样回
        - 上行消息：走 gRPC `OnReceive` 或走批处理 `Coordinator`
    - 成功后回 `UPSTREAM_ACK`

## B. 后端→网关→前端（下行）
1. 后端把 `PushMessage` 发 MQ
2. 网关 consumer 收到后，按 `(bizID, receiverId)` 去 `LinkManager` 找连接
3. `linkEventHandler.OnBackendPushMessage`：
    - 加密 + 编码 + `Link.Send`
    - 启动重传任务，等待前端 `DOWNSTREAM_ACK` 停止重传

## C. 集群层（调度/下线）
- 节点注册 etcd + keepalive
- 周期上报 load
- 优雅下线：停止接入 → 降权/注销 → 获取可用节点 → 连接迁移/关闭

---

# 项目“痛点”（为什么需要这么设计）
## 1) 海量长连接带来的容量与稳定性
- **痛点**：瞬时涌入、发布放量、雪崩（Accept 过快导致 FD/CPU/内存顶满）
- **现有解法**：`TokenLimiter` + ramp-up + backoff 控制接入速度；空闲连接清理；优雅下线

## 2) 消息可靠性（幂等/重传/ACK）
- **痛点**：断网重连、重复发送、下行丢包/乱序
- **现有解法**：
    - 上行幂等（Redis SetNX）
    - 下行 ACK + 重传管理（pushretry）
    - Key 贯穿端到端关联

## 3) 吞吐与后端压力
- **痛点**：每条消息都 gRPC，RPC 数量爆炸、尾延迟上升
- **现有解法**：
    - `Coordinator` 做批量 gRPC
    - 或通过 MQ producer 批量投递削峰填谷

## 4) 多节点路由与连接定位
- **痛点**：后端推送要命中“持有该用户连接的网关节点”（否则本机找不到 link）
- **项目暗示的方向**：
    - 通过注册发现/标签/权重/负载做调度
    - 扩容再均衡与连接迁移（scaleUp + GracefulClose）
- **面试会追问**：如果推送打错节点怎么办？（要么业务侧先查 user->node 映射，要么网关间转发，要么统一通过路由层/一致性哈希）

---

# 面试点（高频可深挖的问题清单）
## 1) 为什么选 `gobwas/ws`？性能点在哪里？
- 零拷贝/更少分配、性能优先的 WS 实现
- 压缩协商（你还打开了 `wsflate/extension.go`）在高吞吐下的 CPU vs 带宽权衡

## 2) Link 抽象的意义？为什么不直接在 handler 里用 conn？
- 把超时、重试、缓冲、限流、关闭语义统一封装
- 便于 LinkManager 做生命周期管理与迁移

## 3) 幂等怎么做？为什么用 `SetNX(bizID-key)`？过期怎么设？
- key 的生成/去重窗口/误判风险
- 失败场景：后端处理成功但 ACK 丢了，是否允许重放？（需要结合业务幂等）

## 4) 下行 ACK + 重传怎么设计？何时停止？最大重试策略？
- 重试间隔、最大次数、客户端 ACK 丢失、客户端重复 ACK 的处理

## 5) 批处理两套方案怎么取舍？
- `Coordinator`（批量 RPC） vs batch_send_to_backend.go（批量 MQ）
- 各自适合：
    - RPC：需要同步响应、强交互
    - MQ：解耦削峰、最终一致

## 6) etcd 注册、租约、load 上报如何用于调度？
- 用 `Load/Capacity/Weight/Labels/Location` 怎么做就近路由/灰度隔离
- 优雅下线为什么要“先降权再迁移再注销”

## 7) 灰度放量为什么用 token + ramp-up + backoff？
- 解决“发布/扩容后瞬间接入导致雪崩”
- backoff 为什么要 reset（成功后重置，避免持续长退避）

---

# 状态总结
- **已完成**：把项目功能模块、系统设计主链路、核心痛点与面试可深挖点按你仓库的真实实现做了结构化梳理。  


# 30 秒概述（面试开场电梯稿）
这是一个 **企业级 IM WebSocket 网关**：前端长连接接入后抽象成 `Link`，由 `LinkManager` 统一管理；所有消息通过 `LinkEventHandler` 链路做 **解密/编解码/限流/幂等/ACK 与重传**，上行可走 **gRPC 同步**或 **MQ 异步（支持批量）** 到业务后端；节点通过 **etcd 注册发现 + load 上报** 支撑多实例水平扩展，并提供 **灰度接入（令牌桶 ramp-up + 退避）**、**扩缩容再均衡** 和 **优雅下线（停止接入→降权→迁移/关闭连接）**。

---

# 3 分钟架构（讲清模块 + 两条主链路）
## 1) 系统分层
- **接入层**：`WebSocketServer` 监听/Upgrade/收包循环，负责启动消费者、注册中心、优雅关闭
- **连接层**：`Link` 封装发送接收、超时/缓冲/重试；`LinkManager` 负责连接生命周期与按用户定位
- **消息处理层**：`linkevent` 里一组 handler 形成处理链
    - `LimitHandler`（消息限流）
    - `Handler/BatchHandler`（解密、幂等、cmd 分发、上行转发、下行 push、ACK、重传）
    - `Coordinator`（按 biz 聚合批量转发）
- **后端交互层**：
    - gRPC：`BackendService.OnReceive`
    - MQ：consumer（push/扩容事件）+ producer（上行事件，含批量）
- **治理层**：etcd 注册、租约 keepalive、load 上报、可用节点查询、优雅下线与迁移
- **容量与稳定性**：接入侧 `TokenLimiter` 灰度放量 + 指数退避；连接空闲清理；单用户/全局限流

## 2) 两条主链路（端到端）
### A. 上行（客户端 → 网关 → 业务后端）
1. `Accept` 前先拿令牌（容量不足则退避拒绝）
2. Upgrade 成 WS，创建 `Link` 纳入 `LinkManager`
3. 收到 payload：
    - 反序列化 `Message` → 解密 `Body`
    - Redis `SetNX(bizID-key)` 幂等去重
    - `Cmd` 分发：心跳回显；上行业务消息转发到后端（gRPC 或批量）；成功回 `UPSTREAM_ACK`

### B. 下行（业务后端 → 网关 → 客户端）
1. 后端发 `PushMessage` 到 MQ
2. 网关 consumer 收到后，用 `(bizID, userID)` 从 `LinkManager` 定位连接
3. 加密 + 编码后 `Link.Send()` 下发；启动重传任务
4. 前端回 `DOWNSTREAM_ACK` 后停止重传

## 3) 多节点与运维
- 节点 `Node{Id, Ip, Port, Weight, Capacity, Load, Labels}` 注册到 etcd，定期上报 `Load=连接数`
- 优雅下线：停止接入、降权/注销、获取其他可用节点，然后迁移/关闭连接

---

# 10 分钟深挖（按“为什么这么做 + 关键细节 + trade-off”讲）
下面是一套你可以照着讲的 10 分钟结构（每段 1-2 分钟），面试官想深入时你也能顺势展开。

## 1) 为什么需要网关（职责边界）
- **目标**：把“长连接接入、协议、安全、可靠性、路由、治理”从业务服务里抽出来
- **边界**：
    - 网关不做业务逻辑，只做协议/连接/投递可靠性与治理
    - 业务后端只关心 `OnReceive`/推送事件消费

可追问点：
- 为什么不用业务服务直接持 WS？（资源模型、扩缩容、发布风险、协议与业务耦合）

## 2) Link / LinkManager 的抽象价值（连接模型）
- **Link**：把 `Send/Receive/Close`、超时、重试、缓冲、压缩状态等封装，避免散落在 handler
- **LinkManager**：
    - 用 `bizID-userID` 生成 `linkID`，便于定位
    - `Len()` 作为节点负载指标（上报/调度）
    - 生命周期：创建/移除/优雅关闭（迁移）

可追问点：
- 为什么用 `(bizID,userID)` 做 linkID？多端登录/多设备怎么办？（可以扩展为 `deviceID`/`connID`）
- 内存连接表的并发安全与性能（sync map / 分片 map）

## 3) 消息协议 Message 的设计（Cmd/Key/Body）
- `Cmd` 把“心跳/上行/ACK/限流提示”等控制语义显式化
- `Key` 贯穿幂等与 ACK 关联
- `Body` 支持加密 + 承载业务 protobuf（网关只当不透明载荷处理）

可追问点：
- Key 的生成策略、重复窗口、过期时间如何选
- 加密/压缩在 CPU 与带宽之间如何权衡（尤其移动端弱网）

## 4) 幂等去重（Redis SetNX）的工程化细节
- **做法**：`SetNX(bizID-key, key, expiration)`
    - 重复上行直接丢弃，避免业务侧重复写入/重复投递
- **关键参数**：
    - `expiration` 是幂等窗口：太短会漏防重，太长占内存且影响重试语义

可追问点（很加分）：
- 如果“业务处理成功但 ACK 丢了”，前端重发会被丢弃，这是否符合业务？
    - 解决方案：ACK 设计、业务幂等、或区分“可重放/不可重放”消息类型

## 5) 下行可靠性：ACK + 重传（pushretry）
- **机制**：下行发送后启动重传任务；收到 `DOWNSTREAM_ACK` 后停止重传
- **设计要点**：
    - 重传间隔、最大次数、超时后策略（告警/降级/丢弃）
    - ACK 幂等（重复 ACK 不应有副作用）

可追问点：
- 如何避免重传风暴（指数退避、抖动、全局限流）
- 客户端重连后 ACK 如何处理（会话/消息状态同步）

## 6) 上行到后端：gRPC vs MQ（取舍与场景）
- **gRPC（同步）**：
    - 优点：请求-响应语义清晰，适合强交互（比如必须立即回执）
    - 风险：RPC 数量爆炸、尾延迟、后端抖动会反噬网关
- **MQ（异步）**：
    - 优点：削峰填谷、解耦、后端可水平扩展消费
    - 风险：最终一致、链路更复杂（幂等/顺序/回执）

可追问点：
- 你项目里为什么两套都保留？（面向不同业务类型或演进路线）

## 7) 批处理：两种批量（Coordinator vs Batch MQ Producer）
你项目里有两类“批”：
- **`Coordinator`（批量 gRPC）**：按 `bizID` 聚合 `OnReceiveRequest`，到阈值/超时触发批调用
- **batch_send_to_backend.go（批量 MQ）**：把多条上行聚合成 JSON `{msgs:[...]}` 一次 Produce

面试讲法（trade-off）：
- gRPC 批：减少 RPC 次数、降低 overhead，仍保留“近实时响应”
- MQ 批：减少 MQ Produce 次数，适合大吞吐异步链路

可追问点（亮点）：
- 批超时/批大小如何调参（吞吐 vs 延迟）
- 批内失败怎么回传、如何定位单条错误
- 批处理 goroutine/锁竞争、内存增长与 backpressure

## 8) 多节点治理：etcd 注册 + load 上报 + 优雅下线
- **Node**：`Weight/Capacity/Load/Labels/Location` 是调度输入
- **优雅下线**：
    - 先停止接入（避免新连接进入）
    - 降权/注销（让路由层不再选中）
    - 获取可用节点列表，引导连接迁移/重连
- **灰度接入**：`TokenLimiter` ramp-up + backoff 防雪崩

可追问点：
- 推送打错节点怎么办？（user->node 映射、路由层一致性哈希、网关间转发）
- load 指标为什么用连接数？能否引入 CPU/内存/消息速率等综合负载？

---

## 你可以直接背的“收尾一句”
这套网关的核心价值是：**用连接抽象 + 事件处理链把协议、安全、可靠性与吞吐治理标准化**；再用 **注册发现、负载上报、灰度接入、优雅下线/扩缩容** 让长连接系统可以像普通微服务一样发布、扩展和回滚。

---

# 状态总结
- **已完成**：给出可直接用于面试的 `30秒/3分钟/10分钟` 三套讲稿结构，并覆盖你项目的关键实现点与可追问深挖点。

---

# 追加：三大重点（可靠传输 / 负载均衡与再均衡 / 自动扩缩容）背诵稿

## 1) 可靠传输（Reliable Delivery）怎么讲
核心一句话：**上行靠“幂等 + 回执”，下行靠“ACK + 重传”，两端都用 `Key` 把全链路串起来。**

### 1.1 上行：幂等 + UPSTREAM_ACK
- **你要解决的故障模型**：弱网/超时导致客户端重发；网关/后端重试导致重复执行。
- **做法**：
  - 前端每条业务消息带全局唯一 `Key`。
  - 网关用 Redis `SetNX(bizID-key)` 做幂等窗口，重复直接丢弃。
  - 后端处理成功后，网关回 `UPSTREAM_ACK(key)`（必要时把后端响应封装在 `Body`）。
- **你可以强调的工程点**：
  - `expiration` 的选择决定了“允许重试”的语义与 Redis 成本。
  - 若业务是强一致/不可丢：要设计“ACK 丢失可补偿”，比如支持查询消息状态或补发 ACK。

### 1.2 下行：DOWNSTREAM_ACK + 重传
- **你要解决的故障模型**：网关到客户端的包可能丢失；客户端收到但 ACK 丢失；客户端重连。
- **做法**：
  - 下行消息也带 `Key`。
  - 网关发送后启动重传任务；客户端收到后回 `DOWNSTREAM_ACK(key)`；网关收到 ACK 停止重传。
- **避免重传风暴**：
  - 指数退避 + 抖动
  - 用户级/全局限流
  - 超过最大次数后降级（转离线、落库等待拉取、告警等）

### 1.3 面试高频追问与回答框架
- **追问：业务处理成功但 UPSTREAM_ACK 丢了怎么办？**
  - **回答框架**：网关幂等保证“业务只执行一次”；ACK 丢了客户端会重发但会被丢弃；若业务需要强可用体验，则补充“状态查询/补偿 ACK/业务侧幂等”。
- **追问：客户端重连后怎么保证下行不丢？**
  - **回答框架**：ACK+重传只能覆盖“在线窗口”；重连后通常要有“离线消息补拉/未 ACK 消息恢复”机制（网关只负责在线投递可靠性，离线一致性交给业务/存储）。

## 2) 负载均衡与再均衡（Load Balance / Rebalance）怎么讲
核心一句话：**IM 的负载均衡对象是“连接/用户会话”，扩缩容时要处理“存量连接”再均衡，而不是只做新连接的轮询。**

### 2.1 调度输入（Node 元信息）
- `Node{Id, Ip, Port, Weight, Capacity, Load, Labels, Location}`
- **Load 的现实含义**：项目里用连接数 `Load = linkManager.Len()` 做快速近似。
- **更真实的负载**：热点用户消息速率、压缩开销、CPU/内存、队列堆积等。

### 2.2 再均衡的三种实现（面试喜欢让你选方案）
- **客户端重定向重连**（推荐讲这个）
  - 网关告诉客户端新节点地址（或下发“重连指令/目标节点列表”）
  - 优点：实现成本适中、易扩展；缺点：需要客户端配合
- **踢下线让客户端重连**
  - 优点：实现最简单；缺点：体验差、重连风暴风险
- **网关间转发/迁移连接**
  - 优点：体验最好；缺点：实现复杂（连接状态迁移、加密上下文、背压与一致性）

### 2.3 追问：推送打错节点怎么办？
- **回答框架**：推送必须先路由到“持有该用户连接”的节点。
  - 方案 1：维护 `user -> node` 映射（注册中心/缓存），推送先查再打
  - 方案 2：在入口层做一致性哈希/就近路由，天然命中
  - 兜底：网关间转发到目标节点（复杂但可救火）

## 3) 自动扩缩容（Auto Scaling）怎么讲
核心一句话：**扩缩容是闭环：用指标触发扩容 → 拉起节点并注册 → 渐进放量 → 再均衡存量连接 → 缩容时优雅下线回收。**

### 3.1 扩容触发指标（你可以按优先级讲）
- **强信号**：连接拒绝率、排队堆积、P99 延迟飙升
- **弱信号**：`Load/Capacity` 接近阈值、CPU/内存持续高位
- **稳定性设计**：冷却时间 + 阈值滞回，避免抖动扩缩容

### 3.2 扩容后的“收敛”为什么重要
- **新节点刚启动就被打爆**是常见事故
- 你项目的典型讲法：
  - 新节点上线后先注册进可用列表
  - 接入侧用 `TokenLimiter.StartRampUp` 渐进放量
  - 容量不足用 `ExponentialBackOff` 临时拒绝新连接保护系统

### 3.3 缩容：一定先优雅下线再回收
- **顺序**：停止接入 → 降权/注销 → 连接迁移/重连 → 再回收实例
- **面试点**：如果直接 kill，会造成大量断线重连，反向把集群打崩

---

# 追加：详细版（可靠传输 / 再均衡 / 扩缩容）10 分钟口述稿

## 0) 讲述路线（10 分钟结构）
- **1 分钟**：可靠传输总览（上行幂等+回执，下行 ACK+重传，Key 贯穿）
- **3 分钟**：上行可靠（幂等窗口、ACK 语义、失败场景与补偿）
- **3 分钟**：下行可靠（ACK、重传策略、重传风暴防护、重连语义）
- **2 分钟**：负载均衡与再均衡（连接级调度、Node 指标、迁移手段）
- **1 分钟**：自动扩缩容闭环（触发→执行→收敛→缩容回收）

你可以把它当成一段连续口播，面试官打断时就从“追问表”切换。

## 1) 可靠传输（1-3 分钟可背稿）
> 我们这个网关的可靠传输分两条链路：**上行**解决“重复执行”和“确认语义”，主要靠 **幂等 + 回执**；**下行**解决“弱网丢包”和“确认到达”，主要靠 **ACK + 重传**。两端用同一个 `Key` 做关联。

### 1.1 上行：幂等 + UPSTREAM_ACK（口播稿）
> 上行场景最大的问题是客户端超时重试、网关/后端重试导致重复处理。我们要求每条上行业务消息都有唯一 `Key`，网关收到后先解包解密，然后用 Redis 做 `SetNX(bizID-key)`，在幂等窗口内同一个 key 只处理一次。接下来把 `Key` 和 `Body` 转发给业务后端（比如 gRPC `OnReceive`），业务处理成功后，网关会下发 `UPSTREAM_ACK(key)` 给客户端，这样客户端能明确知道这条消息已经被网关/后端接收并处理。
>
> 这里的关键点是：我们并不宣称网络层做到 exactly-once，而是做到“客户端可能至少一次发送，但业务侧效果至多一次”，并用 ACK 把结果返回给客户端。

### 1.2 上行关键取舍（你主动补一句就很加分）
- **幂等窗口 expiration 的含义**：
  - 太短：慢重试绕过窗口会重复执行
  - 太长：Redis 成本高，并且会改变业务重试语义
- **ACK 语义**：
  - `UPSTREAM_ACK` 是“已处理/已接收”的确认，还是“已持久化”的确认，要与业务目标一致

### 1.3 下行：ACK + 重传（口播稿）
> 下行场景的问题是弱网丢包、客户端收到但 ACK 丢失、以及短暂断链。我们的做法是下行消息同样带 `Key`，网关每次 push 后会启动重传任务，客户端收到后回 `DOWNSTREAM_ACK(key)`，网关收到 ACK 就停止该 key 的重传。重传策略一般是固定间隔或指数退避，并且有最大次数/总时长，超过阈值后会走降级策略，比如转离线、落库等待补拉、或者告警。
>
> 这样我们把在线窗口内的可靠性做到“近似至少一次投递”，但也会明确：离线后的可靠投递不靠网关硬扛，而是靠业务的离线消息存储与补拉。

### 1.4 下行风暴控制（口播里一句话带过）
- 重传间隔做 **指数退避 + 抖动**
- 叠加 **用户级 / 全局限流**
- 超限后 **熔断降级**，避免被慢客户端拖垮整个网关

## 2) 负载均衡与再均衡（2 分钟可背稿）
> IM 的负载均衡不是请求级的，因为状态在长连接上。我们在 etcd 注册每个节点的 `Node` 信息（`Weight/Capacity/Load/Labels/Location`），并定期上报 `Load`。调度时主要做两件事：第一是新连接的选择（尽量就近、按权重、按负载）；第二是扩缩容时对**存量连接**做再均衡。
>
> 再均衡这块我们一般讲三种手段：最简单的是踢下线让客户端重连；更推荐的是给客户端下发重定向信息，让客户端重连到目标节点；最复杂的是网关间转发或连接迁移，客户端无感但实现成本很高。实际工程里经常是“重定向为主、踢下线兜底”。

### 2.1 面试官一定会问：推送怎么保证命中正确节点？
> 推送必须先路由到“持有该用户连接”的节点，否则本机找不到 link。常见做法是维护 `user->node` 映射（注册中心/缓存），或者入口层一致性哈希保证命中，兜底再做网关间转发。

### 2.2 Load 只用连接数够不够？（你可以这样答）
- 不够精确，但实现简单、稳定、能快速闭环
- 真实负载还和消息速率、压缩开销、热点用户分布、队列堆积相关
- 可演进为“连接数 + QPS + CPU + 内存 + 发送队列长度”的综合权重

## 3) 自动扩缩容（1-2 分钟可背稿）
> 扩缩容必须讲闭环：先用指标触发扩容，比如连接拒绝率、P99 延迟、队列堆积，或者 `Load/Capacity` 接近阈值；然后通过 scaler 拉起新节点，新节点注册到 etcd 进入可用列表。接下来最关键的是**收敛**：新节点不能一上线就吃满连接，所以我们会做渐进放量，比如用令牌桶 ramp-up 控制接入速度；容量不够时用指数退避临时拒绝新连接，保护集群不雪崩。最后，扩容后还要对存量连接做再均衡，否则新节点只吃新连接，见效会很慢。
>
> 缩容时一定先优雅下线：停止接入、降权、迁移/重连，再回收实例，避免直接 kill 引发断线重连风暴。

### 3.1 如何避免扩缩容抖动（答题模板）
- **冷却时间**：扩容后 N 分钟内不再触发下一次
- **滞回阈值**：扩容阈值高、缩容阈值低
- **分组策略**：按 `Location/Labels` 分组扩容，避免跨机房乱扩

---

# 追加：高频追问 → 标准回答（对照表）

## 可靠传输
- **Q：你们是 exactly-once 吗？**
  - A：网络层做不到严格 exactly-once，我们实现的是“至少一次发送 + 幂等保证业务效果至多一次”，并用 ACK 给客户端确认。
- **Q：后端成功但 UPSTREAM_ACK 丢了怎么办？**
  - A：幂等会拦住重复执行；如果业务要求强体验，需要补“状态查询/补偿 ACK/离线结果回推”。
- **Q：下行重传会不会把网关打爆？**
  - A：会控制重传间隔（指数退避+抖动）、最大次数，并做用户级/全局限流，超限降级。

## 负载均衡与再均衡
- **Q：推送怎么保证命中正确网关节点？**
  - A：需要 `user->node` 路由（注册中心/缓存）或一致性哈希；兜底可网关间转发。
- **Q：再均衡怎么做更平滑？**
  - A：优先客户端重定向重连；存量连接按比例/分批迁移；迁移期间停止接入并降权，避免边迁移边变热。

## 自动扩缩容
- **Q：扩容后为什么还需要 ramp-up？**
  - A：防止新节点刚启动缓存未热/资源未稳定就被打满，导致抖动和二次扩容。
- **Q：如何避免扩缩容频繁来回？**
  - A：冷却时间 + 滞回阈值 + 分组扩容策略。