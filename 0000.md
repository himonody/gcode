https://bcnryo9i8z0f.feishu.cn/wiki/RNSnwAxOxiJC9ckmgFVcucOmnJm

https://bcnryo9i8z0f.feishu.cn/wiki/KFa4wp6f2iE6x7khgCvcBv2XnOd

https://bcnryo9i8z0f.feishu.cn/wiki/KHnfwHnJNii2rMk7BQRcdlfkn6c


https://jb170.vip/?t=1771982202916#/

https://blog.csdn.net/2401_86478612/article/details/149844896

Deployment + Service + Ingress，并结合 HPA 实现弹性扩容

第一，在流量接入层（Ingress & Service）：
“所有的外部流量首先通过 Ingress Controller（比如 Nginx Ingress）进入集群。
我在 Ingress 层配置了 HTTPS 证书卸载和基于域名的七层路由转发。
然后，流量会透传给 Service。Service 这里我们用的是 ClusterIP 模式，它主要负责服务发现和负载均衡，通过 Label Selector 找到后端的 Pod，保证即使 Pod IP 变化，服务调用的入口也是稳定的。”

第二，在应用部署层（Deployment）：
“我们的 Go 服务是通过 Deployment 资源进行管理的。
在 Deployment 里，我重点配置了 Resources Requests 和 Limits。这是 HPA 生效的前提，同时也防止某个 Pod 资源泄露影响整个节点。
通常我会把 CPU 的 Limit 设置为 Request 的 2 到 4 倍，给 Go 程序留出突发计算的缓冲空间。”

第三，关于弹性伸缩（HPA）的实现：
“为了应对流量波动，我部署了 Metrics Server 来聚合监控指标，并配置了 HPA (Horizontal Pod Autoscaler)。
策略上，我主要锚定 CPU 利用率。比如当 Pod 的平均 CPU 使用率超过 60% 时，HPA 会自动计算所需的副本数，并修改 Deployment 的 Replicas 字段，触发扩容。
同时，为了防止流量抖动导致频繁扩缩容，我配置了 HPA 的 stabilizationWindow（稳定窗口），通常是缩容冷却 5 分钟。”

最后，为了保证扩缩容过程中的‘零宕机’：
“我在 Deployment 里严格配置了 Readiness Probe（就绪探针）。只有当新扩容的 Pod 通过了健康检查，Service 才会把流量切过去。
同时，在服务代码层面，我们实现了 Graceful Shutdown（优雅停机），拦截 SIGTERM 信号，确保处理完当前请求后再断开连接，避免缩容时出现 502 错误。”


Q: 为什么不用内存做 HPA 指标？

A: “因为 Go 语言的 GC 机制，内存释放有滞后性。内存指标通常只涨不跌，很难反映真实的业务负载，容易导致 HPA 误判一直扩容。CPU 是最直接反映计算压力的指标。”

Q: 如果流量瞬间暴涨（比如秒杀），HPA 来得及吗？

A: “坦白说，HPA 有分钟级的延迟（采集指标+计算+启动容器），对于秒杀场景是来不及的。如果是已知的活动流量，我们会提前通过 CronHPA（定时扩容）或者手动扩容来预热；如果是突发流量，我们会依赖前面的 限流熔断（Rate Limiting） 机制来保命，而不是单纯依赖 HPA。”